{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_processing_and_ml.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nft3Lk3Zl7hj","colab_type":"text"},"source":["# Data Processing an Machine Learning in Python\n","Stephen Casper, scasper@college.harvard.edu"]},{"cell_type":"markdown","metadata":{"id":"YO_Sdb3Jnmrj","colab_type":"text"},"source":["## Imports\n","Not much to it. Numpy is for linear algebra, Sci-kit Learn is for machine learnign, and Matplotlib is for plotting. All three are extremely common."]},{"cell_type":"code","metadata":{"id":"5pHowo8VbOwe","colab_type":"code","colab":{}},"source":["# imports\n","import numpy as np\n","import sklearn as skl\n","from sklearn import datasets\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import tree\n","from sklearn import linear_model\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_hqjXTZUnrKA","colab_type":"text"},"source":["## Getting Data\n","Let's download and process our data a bit. It's the \"Iris\" dataset which consists of 3 classes of flowers. samples is represented as a 4 dimensional vector with dimensions giving the length and width of sepals and petals. We will store our data as numpy arrays where each row is a flower and each column is a feature. We sill also normalize the data. why might this be helpful?"]},{"cell_type":"code","metadata":{"id":"-4yNmXGAc0eQ","colab_type":"code","colab":{}},"source":["# get data\n","iris = datasets.load_iris()\n","x = iris.data\n","y = iris.target\n","\n","# normalize\n","x = x - (np.mean(x, axis=0)) / np.std(x, axis=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-EUBVZboNwK","colab_type":"text"},"source":["## tSNE Visualization\n","Let's process the data and see what structure we can find in it by representing it in 2 dimensions instead of 4. tSNE stands for t-distributed stochastic neighbor embedding, and it's a common stochastic tool for representing high dimensional data in lower dimensions. It iteratively moves poitnts around a space, optimizing for how close neightboring points are and how far distant points are from each other. When you see the results, notice that tSNE is done while blind to the labels. Also, try running this cell multiple times to see what happens. "]},{"cell_type":"code","metadata":{"id":"0CS4X6K7lS-p","colab_type":"code","colab":{}},"source":["# fit tSNE\n","x_tsne = TSNE(n_components=2).fit_transform(x)\n","\n","# get points of each class\n","tsne0 = x_tsne[y==0]\n","tsne1 = x_tsne[y==1]\n","tsne2 = x_tsne[y==2]\n","\n","# plot\n","plt.scatter(tsne0[:,0], tsne0[:,1], c='r')\n","plt.scatter(tsne1[:,0], tsne0[:,1], c='b')\n","plt.scatter(tsne2[:,0], tsne0[:,1], c='g')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVKSoCbHokAK","colab_type":"text"},"source":["## PCA Visualization\n","There's structure! Another technique for dimensionality reduction is principal component analysis (PCA) which represents our data in terms of the axes in design space (in our case it's 4 dimensional space) that explain the most variance in the data. Let's do a PCA transform and plot the first two principal components. Now try running this cell multiple times to see what happens. Also what do you suspect these principal components correspond to?"]},{"cell_type":"code","metadata":{"id":"pvWb3xoEnyui","colab_type":"code","colab":{}},"source":["# fit PCA\n","pca = PCA(n_components=2)\n","x_pca = pca.fit_transform(x) \n","\n","# get points of each class\n","pca0 = x_pca[y==0]\n","pca1 = x_pca[y==1]\n","pca2 = x_pca[y==2]\n","\n","# plot\n","plt.scatter(pca0[:,0], pca0[:,1], c='r')\n","plt.scatter(pca1[:,0], pca1[:,1], c='b')\n","plt.scatter(pca2[:,0], pca2[:,1], c='g')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e41AxvXAzo2i","colab_type":"text"},"source":["## tSNE vs. PCA\n","Which technique, tSNE or PCA, seems to do better at finding low dimensional structure in the dataset? As you noticed, probably tSNE, but PCA has the advantage of returning consistent results. Also, tSNE's performance depends on tuning a fairly arbitary perplexity parameter. For datasets like this, the perplexity doesn't have a large effect, but for others, the perplexity of the model can have a massive effect on the outcomes. See [here](https://https://distill.pub/2016/misread-tsne/). For this reason, tSNE isn't commonly used in published research for the same reason that lie detectors aren't admissible in court--it usually works, but it's not reliable. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"irvAiBj2pC9E","colab_type":"text"},"source":["## Separating Train/Test Data\n","We will use PCA preprocessing for our ML models. And let's put out data into training and testing sets. Why is this a good idea? "]},{"cell_type":"code","metadata":{"id":"7Ev2e66hp9eo","colab_type":"code","colab":{}},"source":["# get x's for each class\n","x0 = x_pca[y==0]\n","x1 = x_pca[y==1]\n","x2 = x_pca[y==2]\n","\n","# make test data with 20 examples from each class\n","test_x = np.concatenate((x0[:20], x1[:20], x2[:20]))\n","test_y = np.concatenate((np.repeat(0, 20), np.repeat(1, 20), np.repeat(2, 20)))\n","\n","# make train data with the rest of the examples\n","n0 = len(x0[20:])\n","n1 = len(x1[20:])\n","n2 = len(x2[20:])\n","train_x = np.concatenate((x0[20:], x1[20:], x2[20:]))\n","train_y = np.concatenate((np.repeat(0, len(x0)-20), np.repeat(1, len(x1)-20), np.repeat(2, len(x2)-20)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8seWja6ArRui","colab_type":"text"},"source":["## KNN Classification\n","Here, we will use the k-nearest neighbor (KNN) classification algorithm which classifies points by labeling them the same as the class representing the plurality of the k nearest neighbors in the training set. "]},{"cell_type":"code","metadata":{"id":"BkUYEwr5rqm7","colab_type":"code","colab":{}},"source":["# fit model\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(train_x, train_y)\n","\n","# print results\n","print('Training data accuracy: ', sum(knn.predict(train_x)==train_y), 'out of', len(train_y))\n","print('Testing data accuracy: ', sum(knn.predict(test_x)==test_y), 'out of', len(test_y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myXiMsIDs1ju","colab_type":"text"},"source":["## Naive Bayes Classification\n","A naive bayes classifier calculates the probability of a datapoint being each class given each of its dimension values and then multiplies together the probabilities for each dimension to find an overall value proportional to the probability for each class. Then it classifies based on the class which has the highest probability estimate. It's called naive because it makes the (bad) assumption that each dimension is independent."]},{"cell_type":"code","metadata":{"id":"20nwJKzBtlVV","colab_type":"code","colab":{}},"source":["# fit model\n","gnb = GaussianNB()\n","gnb.fit(train_x, train_y)\n","\n","# print results\n","print('Training data accuracy: ', sum(gnb.predict(train_x)==train_y), 'out of', len(train_y))\n","print('Testing data accuracy: ', sum(gnb.predict(test_x)==test_y), 'out of', len(test_y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RHP2OrX9qLAk","colab_type":"text"},"source":["Would the naive bayes classifier work better or worse if we used our tSNE processed data instead of our PCA processed data? Why?"]},{"cell_type":"markdown","metadata":{"id":"hxcLcCZLuAk4","colab_type":"text"},"source":["## Decision Tree Classification\n","A decision tree classifier works a lot like a dichotomous key which you may have learned about in biology classes. It asks yes/no questions about each point (i.e. Is the value of the first dimension less than 5?) and proceeds down a fork of the tree accordingly either to a conclusion or another fork. But it only allows itself to ask these questions up to a certain \"depth limit\". Try varying this depth limit. What happens?"]},{"cell_type":"code","metadata":{"id":"0gUvAMWyvcWa","colab_type":"code","colab":{}},"source":["# fit model\n","tree_classifier = tree.DecisionTreeClassifier(max_depth=2)\n","tree_classifier.fit(train_x, train_y)\n","\n","# print results\n","print('Training data accuracy: ', sum(tree_classifier.predict(train_x)==train_y), 'out of', len(train_y))\n","print('Testing data accuracy: ', sum(tree_classifier.predict(test_x)==test_y), 'out of', len(test_y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4W6cLkGRwNXE","colab_type":"text"},"source":["## Linear Classification\n","Finally, we can use a linear classifier which learns a matrix of coefficients for each dimension for each class. It then classifies based on what class value is the highest when that coefficient matrix is multiplied with each datapoint vector. Ours will be trained using a technique called gradient descent. Try varying the number of maximum training steps to take. "]},{"cell_type":"code","metadata":{"id":"mNSvtauMxcb_","colab_type":"code","colab":{}},"source":["# fit model\n","lm = linear_model.SGDClassifier(max_iter=1)\n","lm.fit(train_x, train_y)\n","\n","# print results\n","print('Training data accuracy: ', sum(lm.predict(train_x)==train_y), 'out of', len(train_y))\n","print('Testing data accuracy: ', sum(lm.predict(test_x)==test_y), 'out of', len(test_y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vtkWxuEdbWCi","colab_type":"text"},"source":["## Try Something New\n","Pick one or two:\n","\n","- Find a new dataset from sklearn and train a model on it.\n","- Find a new classification algorithm and implement it. \n","- Implement an alorithm from scratch.\n","- Search for the sklearn info page on one of the algorithms and find parameters to tune and see how it affects performance. \n","- Find a new way to preprocess data and see how it affects performance with one or more of the algorithms. \n","- Learn more about the math behind one of these algorithms (other than KNN). You might be expecially interested in learning about gradient descent. \n"]}]}